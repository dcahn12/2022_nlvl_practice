{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf24ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import errno\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import data\n",
    "from config import cfg\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "# magic keyword for tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba633e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "622d4cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "ENGINE_STAGE : TRAINER\n",
      "------------------------------------------------------------------------------------------\n",
      "SENTENCE : {'MIN_COUNT': 1, 'TRAIN_MAX_LENGTH': 30, 'TEST_MAX_LENGTH': 30}\n",
      "------------------------------------------------------------------------------------------\n",
      "DYNAMIC_FILTER : {'MODEL': 'GRU', 'POOLING': 'MeanPoolingLayer', 'HEAD_MODEL': 'MLP', 'TAIL_MODEL': 'GRU', 'GRU': {'NUM_LAYERS': 1, 'HIDDEN_SIZE': 256, 'BIAS': False, 'BIDIRECTIONAL': True, 'BATCH_FIRST': True, 'DROPOUT': 0.0}, 'MLP': {'INPUT_DIM': 512, 'OUTPUT_DIM': 512}}\n",
      "------------------------------------------------------------------------------------------\n",
      "REDUCTION : {'INPUT_SIZE': 1024, 'OUTPUT_SIZE': 512}\n",
      "------------------------------------------------------------------------------------------\n",
      "LOCALIZATION : {'INPUT_SIZE': 512, 'HIDDEN_SIZE': 256, 'NUM_LAYERS': 2, 'BIAS': False, 'DROPOUT': 0.5, 'BIDIRECTIONAL': True, 'BATCH_FIRST': True}\n",
      "------------------------------------------------------------------------------------------\n",
      "CLASSIFICATION : {'INPUT_SIZE': 512, 'OUTPUT_SIZE': 1}\n",
      "------------------------------------------------------------------------------------------\n",
      "DATASETS : {'TRAIN': 'charades_sta_train', 'TRAIN_SAMPLES': 12404.0, 'TEST': 'charades_sta_test', 'TEST_SAMPLES': 3720.0}\n",
      "------------------------------------------------------------------------------------------\n",
      "BATCH_SIZE_TRAIN : 256\n",
      "------------------------------------------------------------------------------------------\n",
      "BATCH_SIZE_TEST : 256\n",
      "------------------------------------------------------------------------------------------\n",
      "SOLVER : {'TYPE': 'ADAM', 'BASE_LR': 0.0001, 'WEIGHT_DECAY': 1e-05, 'EPSILON': '1E-8'}\n",
      "------------------------------------------------------------------------------------------\n",
      "EPOCHS : 50\n"
     ]
    }
   ],
   "source": [
    "config_path = \"/root/TMLGA_refactoring/experiments/charades_sta_train.yaml\"\n",
    "with open(config_path) as f:\n",
    "    config_file = yaml.full_load(f)\n",
    "for item, doc in config_file.items():\n",
    "    print(\"---\"*30)\n",
    "    print(item, \":\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beef823",
   "metadata": {},
   "source": [
    "## Miscellaneuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc6194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a83fcf",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2958c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.rnns import feed_forward_rnn\n",
    "from utils.gru import GRU\n",
    "from utils.mlp import MLP\n",
    "from utils.pooling import MeanPoolingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a0c15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicFilter(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DynamicFilter, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.tail_df = GRU(cfg)\n",
    "        self.pooling_layer = MeanPoolingLayer()\n",
    "        self.head_df = MLP(cfg)\n",
    "\n",
    "    def forward(self, sequences, lengths=None):\n",
    "        output, _ = self.tail_df(sequences, lengths)\n",
    "        output = self.pooling_layer(output, lengths)\n",
    "        output = self.head_df(output)\n",
    "        return output, lengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "397dfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLVL(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(NLVL, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.batch_size = cfg.BATCH_SIZE_TRAIN\n",
    "        self.model_df  = DynamicFilter(cfg)\n",
    "\n",
    "        self.reduction  = nn.Linear(cfg.REDUCTION.INPUT_SIZE, cfg.REDUCTION.OUTPUT_SIZE)\n",
    "        self.multimodal_fc1 = nn.Linear(512*2, 1)\n",
    "        self.multimodal_fc2 = nn.Linear(512, 1)\n",
    "\n",
    "        self.rnn_localization = nn.GRU(input_size   = cfg.LOCALIZATION.INPUT_SIZE,\n",
    "                                        hidden_size  = cfg.LOCALIZATION.HIDDEN_SIZE,\n",
    "                                        num_layers   = cfg.LOCALIZATION.NUM_LAYERS,\n",
    "                                        bias         = cfg.LOCALIZATION.BIAS,\n",
    "                                        dropout      = cfg.LOCALIZATION.DROPOUT,\n",
    "                                        bidirectional= cfg.LOCALIZATION.BIDIRECTIONAL,\n",
    "                                        batch_first = cfg.LOCALIZATION.BATCH_FIRST)\n",
    "\n",
    "        self.pooling = MeanPoolingLayer()\n",
    "        \n",
    "        self.starting = nn.Linear(cfg.CLASSIFICATION.INPUT_SIZE, cfg.CLASSIFICATION.OUTPUT_SIZE) # 512 -> 1\n",
    "        self.ending = nn.Linear(cfg.CLASSIFICATION.INPUT_SIZE, cfg.CLASSIFICATION.OUTPUT_SIZE)   # 512 -> 1\n",
    "\n",
    "    def attention(self, videoFeat, filter, lengths):\n",
    "        pred_local = torch.bmm(videoFeat, filter.unsqueeze(2)).squeeze()\n",
    "        return pred_local\n",
    "\n",
    "    def get_mask_from_sequence_lengths(self, sequence_lengths: torch.Tensor, max_length: int):\n",
    "        ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)\n",
    "        range_tensor = ones.cumsum(dim=1)\n",
    "        return (sequence_lengths.unsqueeze(1) >= range_tensor).long()\n",
    "\n",
    "    def masked_softmax(self, vector: torch.Tensor, mask: torch.Tensor, dim: int = -1, memory_efficient: bool = False, mask_fill_value: float = -1e32):\n",
    "        if mask is None:\n",
    "            result = torch.nn.functional.softmax(vector, dim=dim)\n",
    "        else:\n",
    "            mask = mask.float()\n",
    "            while mask.dim() < vector.dim():\n",
    "                mask = mask.unsqueeze(1)\n",
    "            if not memory_efficient:\n",
    "                # To limit numerical errors from large vector elements outside the mask, we zero these out.\n",
    "                result = torch.nn.functional.softmax(vector * mask, dim=dim)\n",
    "                result = result * mask\n",
    "                result = result / (result.sum(dim=dim, keepdim=True) + 1e-13)\n",
    "            else:\n",
    "                masked_vector = vector.masked_fill((1 - mask).byte(), mask_fill_value)\n",
    "                result = torch.nn.functional.softmax(masked_vector, dim=dim)\n",
    "\n",
    "        return result + 1e-13\n",
    "\n",
    "    def mask_softmax(self, feat, mask):\n",
    "        return self.masked_softmax(feat, mask, memory_efficient=False)\n",
    "\n",
    "    def kl_div(self, p, gt, length):\n",
    "        individual_loss = []\n",
    "        for i in range(length.size(0)):\n",
    "            vlength = int(length[i])\n",
    "            ret = gt[i][:vlength] * torch.log(p[i][:vlength]/gt[i][:vlength])\n",
    "            individual_loss.append(-torch.sum(ret))\n",
    "        individual_loss = torch.stack(individual_loss)\n",
    "        return torch.mean(individual_loss)\n",
    "\n",
    "    def forward(self, videoFeat, videoFeat_lengths, tokens, tokens_lengths, start, end, localiz):\n",
    "\n",
    "        mask = self.get_mask_from_sequence_lengths(videoFeat_lengths, int(videoFeat.shape[1]))\n",
    "\n",
    "        filter_start, lengths = self.model_df(tokens, tokens_lengths)\n",
    "\n",
    "        videoFeat   = self.reduction(videoFeat)\n",
    "\n",
    "        attention = self.attention(videoFeat, filter_start, lengths)\n",
    "        rqrt_length = torch.rsqrt(lengths.float()).unsqueeze(1).repeat(1, attention.shape[1])\n",
    "        attention = attention * rqrt_length\n",
    "\n",
    "        attention = self.mask_softmax(attention, mask)\n",
    "        videoFeat_hat = attention.unsqueeze(2).repeat(1,1,self.cfg.REDUCTION.OUTPUT_SIZE) * videoFeat\n",
    "        output, _ = feed_forward_rnn(self.rnn_localization, videoFeat_hat, lengths=videoFeat_lengths)\n",
    "\n",
    "\n",
    "        pred_start = self.starting(output.view(-1, output.size(2))).view(-1,output.size(1),1).squeeze()\n",
    "        pred_start = self.mask_softmax(pred_start, mask)\n",
    "\n",
    "        pred_end = self.ending(output.view(-1, output.size(2))).view(-1,output.size(1),1).squeeze()\n",
    "        pred_end = self.mask_softmax(pred_end, mask)\n",
    "\n",
    "        start_loss = self.kl_div(pred_start, start, videoFeat_lengths)\n",
    "        end_loss   = self.kl_div(pred_end, end, videoFeat_lengths)\n",
    "\n",
    "        atten_loss = torch.sum(-( (1-localiz) * torch.log((1-attention) + 1E-12)), dim=1)\n",
    "        atten_loss = torch.mean(atten_loss)\n",
    "\n",
    "        total_loss = start_loss + end_loss + atten_loss\n",
    "\n",
    "        return total_loss, pred_start, pred_end, attention, atten_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e347d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "def make_optimizer(cfg, model):\n",
    "    params = []\n",
    "    for key, value in model.named_parameters():\n",
    "        if not value.requires_grad:\n",
    "            continue\n",
    "        lr = cfg.SOLVER.BASE_LR\n",
    "        weight_decay = cfg.SOLVER.WEIGHT_DECAY\n",
    "        if \"bias\" in key:\n",
    "            lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR\n",
    "            weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS\n",
    "        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n",
    "\n",
    "    if cfg.SOLVER.TYPE == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(params, lr, momentum=cfg.SOLVER.MOMENTUM, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n",
    "    elif cfg.SOLVER.TYPE == \"ADAM\":\n",
    "        optimizer = torch.optim.Adam(params, lr, eps=cfg.SOLVER.EPSILON, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58de6a",
   "metadata": {},
   "source": [
    "## Metric tIoU class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e48f33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricIoU(object):\n",
    "    def __init__(self, cfg, dataset_size, is_train=True):\n",
    "\n",
    "        self.loss = []\n",
    "        self.IoU  = []\n",
    "        self.mIoU = []\n",
    "        self.aux_mIoU = []\n",
    "        self.vis_dir = \"{}{}\".format(cfg.VISUALIZATION_DIRECTORY, cfg.EXPERIMENT_NAME)\n",
    "        mkdir(self.vis_dir)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if is_train == True:\n",
    "            self.state = \"training\"\n",
    "        else:\n",
    "            self.state = \"testing\"\n",
    "\n",
    "    def tIoU(self, start, end, pred_start, pred_end):\n",
    "        tt1 = np.maximum(start, pred_start)\n",
    "        tt2 = np.minimum(end, pred_end)\n",
    "        # Intersection including Non-negative overlap score.\n",
    "        segments_intersection = (tt2 - tt1).clip(0)\n",
    "        # Segment union.\n",
    "        segments_union = (pred_end - pred_start) \\\n",
    "          + (end - start) - segments_intersection\n",
    "        # Compute overlap as the ratio of the intersection\n",
    "        # over union of two segments.\n",
    "        tIoU = segments_intersection.astype(float) / segments_union\n",
    "        return tIoU\n",
    "\n",
    "    def run(self, indexes, pred_start, pred_end, start, end, loss, time_starts, time_ends, factors, fps):\n",
    "        l = loss.detach().item()\n",
    "        self.loss.append(l)\n",
    "\n",
    "        startings = np.argmax(pred_start.detach().cpu().numpy(), axis=1)\n",
    "        endings   = np.argmax(pred_end.detach().cpu().numpy(), axis=1)\n",
    "        \n",
    "        startings = factors * (startings) / fps\n",
    "        endings = factors * (endings + 1) / fps\n",
    "\n",
    "        gt_start = np.array(time_starts)\n",
    "        gt_end   = np.array(time_ends)\n",
    "        \n",
    "        iou = self.tIoU(gt_start, gt_end, startings, endings)\n",
    "        self.IoU.append(iou)\n",
    "        mIoU = np.mean(iou)\n",
    "        self.mIoU.append(mIoU)\n",
    "\n",
    "    def iou_print(self):\n",
    "        new_ious = []\n",
    "\n",
    "        for batch in self.IoU:\n",
    "            for p in batch:\n",
    "                new_ious.append(p)\n",
    "\n",
    "        th = {0.1: 0, 0.3: 0, 0.5: 0, 0.7: 0}\n",
    "        for i in range(len(new_ious)):\n",
    "            for k in th.keys():\n",
    "                if round(new_ious[i],2) >= k:\n",
    "                    th[k] += 1\n",
    "\n",
    "        if self.state == \"training\":\n",
    "            a = {str(k): round(v * 100 / self.cfg.DATASETS.TRAIN_SAMPLES,2) for k, v in th.items()}\n",
    "        else:\n",
    "            a = {str(k): round(v * 100 / self.cfg.DATASETS.TEST_SAMPLES,2) for k, v in th.items()}\n",
    "\n",
    "        self.IoU = []\n",
    "        self.mIoU = []\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116278e7",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e2d3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(cfg):\n",
    "    print('trainer')\n",
    "    # Dataloader load\n",
    "    dataloader_train, dataset_size_train = data.make_dataloader(cfg, is_train=True)\n",
    "    dataloader_test, dataset_size_test   = data.make_dataloader(cfg, is_train=False)\n",
    "\n",
    "    # Model & Optimizer (default: Adam with weight decay)\n",
    "    model = NLVL(cfg)\n",
    "    model.cuda()\n",
    "    optimizer = make_optimizer(cfg, model)\n",
    "\n",
    "    # Metric tIoU\n",
    "    tiou_train = MetricIoU(cfg, dataset_size_train)\n",
    "    tiou_test  = MetricIoU(cfg, dataset_size_test, is_train=False)\n",
    "\n",
    "    # Tensorboard logging\n",
    "    writer_path = os.path.join(cfg.VISUALIZATION_DIRECTORY, cfg.EXPERIMENT_NAME)\n",
    "    writer = SummaryWriter(writer_path)\n",
    "\n",
    "    total_iterations = 0\n",
    "    total_iterations_val = 0\n",
    "\n",
    "    for epoch in range(cfg.EPOCHS):\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        # Training\n",
    "        model.train()\n",
    "        for iteration, batch in enumerate(dataloader_train):\n",
    "            index     = batch[0]\n",
    "\n",
    "            videoFeat = batch[1].cuda()\n",
    "            videoFeat_lengths = batch[2].cuda()\n",
    "            tokens         = batch[3].cuda()\n",
    "            tokens_lengths = batch[4].cuda()\n",
    "            start    = batch[5].cuda()\n",
    "            end      = batch[6].cuda()\n",
    "            localiz  = batch[7].cuda()\n",
    "            \n",
    "            localiz_lengths = batch[8]\n",
    "            time_starts = batch[9]\n",
    "            time_ends = batch[10]\n",
    "            factors = batch[11]\n",
    "            fps = batch[12]\n",
    "\n",
    "            loss, pred_start, pred_end, attention, atten_loss = model(videoFeat, videoFeat_lengths, tokens, tokens_lengths, start, end, localiz)\n",
    "            \n",
    "            if iteration % 10 == 0 or iteration == len(dataloader_train)-1:\n",
    "                print(\"Epoch: {} / Iteration: {}/{} / Loss :{}\".format(str(epoch), str(iteration), str(len(dataloader_train)), loss))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "\n",
    "            tiou_train.run(index, pred_start, pred_end, start, end, loss.detach(), time_starts, time_ends, factors, fps)\n",
    "\n",
    "            writer.add_scalar(f'mlnlp/Progress_Loss', loss.item(), total_iterations)\n",
    "            writer.add_scalar(f'mlnlp/Progress_Attention_Loss', atten_loss.item(), total_iterations)\n",
    "\n",
    "            total_iterations += 1.\n",
    "\n",
    "        print(\"Epoch: {} / Total Loss: {}\".format(str(epoch), np.mean(tiou_train.loss)))\n",
    "        writer.add_scalar(f'mlnlp/Train_Loss', np.mean(tiou_train.loss), epoch)\n",
    "        writer.add_scalar(f'mlnlp/Train_Mean_IoU', np.mean(tiou_train.mIoU), epoch)\n",
    "\n",
    "        tiou_train.iou_print()\n",
    "        torch.save(model, \"./checkpoints/{}/model_epoch_{}\".format(cfg.EXPERIMENT_NAME,epoch))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        for iteration, batch in enumerate(dataloader_test):\n",
    "            index     = batch[0]\n",
    "\n",
    "            videoFeat = batch[1].cuda()\n",
    "            videoFeat_lengths = batch[2].cuda()\n",
    "            tokens         = batch[3].cuda()\n",
    "            tokens_lengths = batch[4].cuda()\n",
    "            start    = batch[5].cuda()\n",
    "            end      = batch[6].cuda()\n",
    "            localiz  = batch[7].cuda()\n",
    "            \n",
    "            localiz_lengths = batch[8]\n",
    "            time_starts = batch[9]\n",
    "            time_ends = batch[10]\n",
    "            factors = batch[11]\n",
    "            fps = batch[12]\n",
    "\n",
    "            loss, pred_start, pred_end, attention,atten_loss = model(videoFeat, videoFeat_lengths, tokens, tokens_lengths, start, end, localiz)\n",
    "            tiou_test.run(index, pred_start, pred_end, start, end, loss.detach(), time_starts, time_ends, factors, fps)\n",
    "\n",
    "            total_iterations_val += 1\n",
    "\n",
    "        writer.add_scalar(f'mlnlp/Valid_Loss', np.mean(tiou_test.loss), epoch)\n",
    "        writer.add_scalar(f'mlnlp/Valid_Mean_IoU', np.mean(tiou_test.mIoU), epoch)\n",
    "        writer.add_scalars(f'mlnlp/Valid_tIoU_th', tiou_test.iou_print(), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a515732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(cfg):\n",
    "    print('testing')\n",
    "    # Dataloader\n",
    "    dataloader_test, dataset_size_test   = data.make_dataloader(cfg, is_train=False)\n",
    "\n",
    "    # Model & Pretrained checkpoint load\n",
    "    model = NLVL(cfg)\n",
    "\n",
    "    if cfg.TEST.MODEL.startswith('.'):\n",
    "        load_path = cfg.TEST.MODEL.replace(\".\", os.path.realpath(\".\"))\n",
    "    else:\n",
    "        load_path = cfg.TEST.MODEL\n",
    "\n",
    "    model = torch.load(load_path)\n",
    "    model.cuda()\n",
    "    \n",
    "    # Metric tIoU\n",
    "    tiou_test  = MetricIoU(cfg, dataset_size_test, is_train=False)\n",
    "\n",
    "    # Tensorboard logging\n",
    "    writer_path = os.path.join(cfg.VISUALIZATION_DIRECTORY, cfg.EXPERIMENT_NAME)\n",
    "    writer = SummaryWriter(writer_path)\n",
    "\n",
    "    total_iterations = 0\n",
    "    total_iterations_val = 0\n",
    "\n",
    "    model.eval()\n",
    "    epoch = 1\n",
    "    for iteration, batch in enumerate(dataloader_test):\n",
    "        index     = batch[0]\n",
    "\n",
    "        videoFeat = batch[1].cuda()\n",
    "        videoFeat_lengths = batch[2].cuda()\n",
    "        tokens         = batch[3].cuda()\n",
    "        tokens_lengths = batch[4].cuda()\n",
    "        start    = batch[5].cuda()\n",
    "        end      = batch[6].cuda()\n",
    "        localiz  = batch[7].cuda()\n",
    "        \n",
    "        localiz_lengths = batch[8]\n",
    "        time_starts = batch[9]\n",
    "        time_ends = batch[10]\n",
    "        factors = batch[11]\n",
    "        fps = batch[12]\n",
    "\n",
    "        loss, pred_start, pred_end, attention, atten_loss = model(videoFeat, videoFeat_lengths, tokens, tokens_lengths, start, end, localiz)\n",
    "        tiou_test.run(index, pred_start, pred_end, start, end, loss.detach(), time_starts, time_ends, factors, fps)\n",
    "        total_iterations_val += 1\n",
    "    \n",
    "    tiou_test.iou_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f3b200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_path):\n",
    "    parser = argparse.ArgumentParser(description=\"TMLGA\")\n",
    "    parser.add_argument(\n",
    "        \"--config-file\",\n",
    "        default=config_path,\n",
    "        type=str,)\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    experiment_name = args.config_file.split(\"/\")[-1]\n",
    "    log_directory   = args.config_file.replace(experiment_name,\"logs/\")\n",
    "    vis_directory   = args.config_file.replace(experiment_name,\"visualization/\")\n",
    "    experiment_name = experiment_name.replace(\".yaml\",\"\")\n",
    "    \n",
    "    cfg.merge_from_list(['EXPERIMENT_NAME', experiment_name, 'LOG_DIRECTORY', log_directory, \"VISUALIZATION_DIRECTORY\", vis_directory])\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "\n",
    "    output_dir = \"./{}\".format(cfg.LOG_DIRECTORY)\n",
    "\n",
    "    # reproductibility\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if cfg.ENGINE_STAGE == \"TRAINER\":\n",
    "        trainer(cfg)\n",
    "    elif cfg.ENGINE_STAGE == \"TESTER\":\n",
    "        tester(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd0f8fe1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer\n",
      "True\n",
      "loading annotations into memory... Done (t=0.10s)\n",
      "charades_vocab_1_30.pickle True\n",
      "Creating index.. {'id': 8251, 'video': 'LEOL6', 'time_start': 8.0, 'time_end': 6.25, 'frame_start': 193.28, 'frame_end': 151.0, 'feature_start': 23.04, 'feature_end': 17, 'description': 'person is holding medicine.', 'number_features': 18, 'number_frames': 151, 'fps': 24.16, 'label': 107, 'tokens': ['person', 'is', 'holding', 'medicine', '.'], 'preprocessing': 'edison_v1'}\n",
      "{'id': 8439, 'video': 'IOL8Q', 'time_start': 20.9, 'time_end': 12.0, 'frame_start': 502.20539828834757, 'frame_end': 288.3475971033575, 'feature_start': 59.8518762343647, 'feature_end': 34.364713627386436, 'description': 'a person is throwing a bag out of the room.', 'number_features': 87, 'number_frames': 730, 'fps': 24.02896642527979, 'label': 24, 'tokens': ['a', 'person', 'is', 'throwing', 'a', 'bag', 'out', 'of', 'the', 'room', '.'], 'preprocessing': 'edison_v1'}\n",
      "{'id': 9622, 'video': 'AKKWU', 'time_start': 38.0, 'time_end': 12.17, 'frame_start': 914.8726376335251, 'frame_end': 293.0, 'feature_start': 109.28512736236647, 'feature_end': 34, 'description': 'person opens the cabinet.', 'number_features': 35, 'number_frames': 293, 'fps': 24.075595727198028, 'label': 113, 'tokens': ['person', 'opens', 'the', 'cabinet', '.'], 'preprocessing': 'edison_v1'}\n",
      "{'id': 9623, 'video': 'AKKWU', 'time_start': 38.0, 'time_end': 12.17, 'frame_start': 914.8726376335251, 'frame_end': 293.0, 'feature_start': 109.28512736236647, 'feature_end': 34, 'description': 'person has opened a cabinet.', 'number_features': 35, 'number_frames': 293, 'fps': 24.075595727198028, 'label': 113, 'tokens': ['person', 'has', 'opened', 'a', 'cabinet', '.'], 'preprocessing': 'edison_v1'}\n",
      " Ok! 12404\n",
      "False\n",
      "loading annotations into memory... Done (t=0.05s)\n",
      "charades_vocab_1_30.pickle True\n",
      "Cargando vocab\n",
      "Creating index..  Ok! 3720\n",
      "Epoch 0\n",
      "Epoch: 0 / Iteration: 0/49 / Loss :7.259546279907227\n",
      "Epoch: 0 / Iteration: 10/49 / Loss :7.346736431121826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e1ea5f6f84d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/root/TMLGA_refactoring/experiments/charades_sta_train.yaml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-986de523b995>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENGINE_STAGE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"TRAINER\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENGINE_STAGE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"TESTER\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-467f322727be>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mindex\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mvideoFeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_path = \"/root/TMLGA_refactoring/experiments/charades_sta_train.yaml\"\n",
    "main(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498df30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a192e045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 408621), started 0:00:02 ago. (Use '!kill 408621' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"experiments/visualization/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa2169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
